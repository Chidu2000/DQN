{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c008692-2218-4a1a-8f65-6a9c91a8ae7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax \n",
    "from jax import numpy as jnp\n",
    "import chex\n",
    "import gymnax\n",
    "from gymnax.environments.environment import EnvState, Environment, EnvParams\n",
    "\n",
    "from typing import Tuple, Any, Callable\n",
    "from functools import partial\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5d7da67-60ff-42d2-80a9-37d6cde9ee34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import (\n",
    "    DQNTrainingArgs, DQNTrainState,\n",
    "    DQN, DQNParameters, DQNAgent,\n",
    "    select_action, compute_loss, update_target,\n",
    "    initialize_agent_state,\n",
    "    compute_loss_double_dqn,\n",
    "    SimpleDQNAgent,\n",
    "    DoubleDQNAgent\n",
    ")\n",
    "from buffer import ReplayBuffer, ReplayBufferStorage, FIFOBuffer\n",
    "from trainer import agent_iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bdd1c0bd-f5cc-452d-8799-85453efc63ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds_dqn = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2724fb87-f9f9-45f7-8eae-0c97f5c6a426",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/11 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/11 [00:07<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "All input arrays must have the same shape.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_2960\\2597350756.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[0menvironment_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mjnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[0magent_iter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpartial\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0magent_iteration\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSimpleDQNAgent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mFIFOBuffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv_reset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv_step\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m     agent_iter = jax.jit(agent_iter, donate_argnums=(2,)).lower(\n\u001b[0m\u001b[0;32m     23\u001b[0m         \u001b[1;31m# states\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[0mrng\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0magent_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffer_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv_state\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[1;31m# inputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "    \u001b[1;31m[... skipping hidden 12 frame]\u001b[0m\n",
      "\u001b[1;32mf:\\course-work MS\\Reinforcement Learning\\DQN\\trainer.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(args, agent, buffer, env_reset, env_step, rng, agent_state, buffer_state, env_state, last_obs, environment_step)\u001b[0m\n\u001b[0;32m    150\u001b[0m     \u001b[0mscan_inital_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mrng\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0magent_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffer_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlast_obs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menvironment_step\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    151\u001b[0m     \u001b[0menv_steps_per_train_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_batch_size\u001b[0m \u001b[1;33m//\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_intensity\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m     \u001b[0mtrain_steps_per_target_upd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget_update_every\u001b[0m \u001b[1;33m//\u001b[0m \u001b[0menv_steps_per_train_step\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    153\u001b[0m     \u001b[0magent_update_steps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_steps_per_target_upd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 154\u001b[1;33m     \u001b[0mscan_final_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdqn_losses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjax\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlax\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscan\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mupdate_step\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscan_inital_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0magent_update_steps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    155\u001b[0m     \u001b[1;33m(\u001b[0m\u001b[0mrng\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0magent_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffer_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlast_obs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menvironment_step\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscan_final_state\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m     \u001b[1;31m# run several evaluation episodes and compute returns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    157\u001b[0m     rng, eval_returns = eval_agent(\n",
      "    \u001b[1;31m[... skipping hidden 9 frame]\u001b[0m\n",
      "\u001b[1;32mf:\\course-work MS\\Reinforcement Learning\\DQN\\trainer.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(scan_state, t)\u001b[0m\n\u001b[0;32m    142\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mupdate_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscan_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m         \u001b[0mrng\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0magent_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffer_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlast_obs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menvironment_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscan_state\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 144\u001b[1;33m         rng, agent_state, buffer_state, env_state, obs, environment_step, dqn_loss = agent_update_step(\n\u001b[0m\u001b[0;32m    145\u001b[0m             \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv_step\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    146\u001b[0m             \u001b[0mrng\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0magent_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffer_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlast_obs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menvironment_step\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    147\u001b[0m         )\n",
      "\u001b[1;32mf:\\course-work MS\\Reinforcement Learning\\DQN\\trainer.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(args, agent, buffer, env_step, rng, agent_state, buffer_state, env_state, last_obs, environment_step)\u001b[0m\n\u001b[0;32m     46\u001b[0m     \u001b[0meps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0meps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mend_eps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menv_steps_per_train_step\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[0mrng\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstep_rng\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction_rng\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjax\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrng\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m         \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdqn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction_rng\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0magent_state\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m         \u001b[0mnew_obs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep_rng\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     51\u001b[0m         \u001b[0mbuffer_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_transition\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbuffer_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnew_obs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m         \u001b[0mobs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_obs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[0mlast_obs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_2960\\2597350756.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(rng, env_state, action)\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0menv_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrng\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mchex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPRNGKey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv_state\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mEnvState\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mchex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mArray\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTuple\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mchex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mArray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mEnvState\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrng\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "    \u001b[1;31m[... skipping hidden 11 frame]\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\User\\anaconda3\\Lib\\site-packages\\gymnax\\environments\\environment.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, key, state, action, params)\u001b[0m\n\u001b[0;32m     41\u001b[0m         \u001b[1;31m# Use default env parameters if no others specified\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mparams\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m             \u001b[0mparams\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdefault_params\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m         \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey_reset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjax\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 45\u001b[1;33m         \u001b[0mobs_st\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate_st\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep_env\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     46\u001b[0m         \u001b[0mobs_re\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate_re\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_env\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey_reset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m         \u001b[1;31m# Auto-reset environment based on termination\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         state = jax.tree_map(\n",
      "\u001b[1;32mc:\\Users\\User\\anaconda3\\Lib\\site-packages\\gymnax\\environments\\classic_control\\cartpole.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, key, state, action, params)\u001b[0m\n\u001b[0;32m     93\u001b[0m         )\n\u001b[0;32m     94\u001b[0m         \u001b[0mdone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_terminal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m         return (\n\u001b[1;32m---> 97\u001b[1;33m             \u001b[0mlax\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop_gradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_obs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     98\u001b[0m             \u001b[0mlax\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop_gradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     99\u001b[0m             \u001b[0mjnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreward\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m             \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\User\\anaconda3\\Lib\\site-packages\\gymnax\\environments\\classic_control\\cartpole.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, state, params, key)\u001b[0m\n\u001b[0;32m    118\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_obs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mEnvState\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mchex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mArray\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m         \u001b[1;34m\"\"\"Applies observation function to state.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 120\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mjnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx_dot\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtheta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtheta_dot\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\User\\anaconda3\\Lib\\site-packages\\jax\\_src\\numpy\\lax_numpy.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(object, dtype, copy, order, ndmin, device)\u001b[0m\n\u001b[0;32m   5413\u001b[0m     \u001b[1;32massert\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maval\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5414\u001b[0m     \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_array_copy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mcopy\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5415\u001b[0m   \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5416\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5417\u001b[1;33m       \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0melt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0melt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5418\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5419\u001b[0m       \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5420\u001b[0m   \u001b[1;32melif\u001b[0m \u001b[0m_supports_buffer_protocol\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\User\\anaconda3\\Lib\\site-packages\\jax\\_src\\numpy\\lax_numpy.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(arrays, axis, out, dtype)\u001b[0m\n\u001b[0;32m   4470\u001b[0m     \u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_canonicalize_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4471\u001b[0m     \u001b[0mnew_arrays\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4472\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4473\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mshape0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4474\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"All input arrays must have the same shape.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4475\u001b[0m       \u001b[0mnew_arrays\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4476\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_arrays\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: All input arrays must have the same shape."
     ]
    }
   ],
   "source": [
    "for SEED in tqdm(range(40, 51)):\n",
    "    args = DQNTrainingArgs()\n",
    "    rng = jax.random.key(SEED)\n",
    "    rng, agent_init_rng = jax.random.split(rng, 2)\n",
    "    # create the agent and its optimization state\n",
    "    agent_state = SimpleDQNAgent.initialize_agent_state(SimpleDQNAgent.dqn, agent_init_rng, args)\n",
    "    # create the environment\n",
    "    env, env_params = gymnax.make('CartPole-v1')\n",
    "    def env_reset(rng: chex.PRNGKey) -> Tuple[chex.Array, EnvState]:\n",
    "        return env.reset(rng, env_params)\n",
    "    def env_step(rng: chex.PRNGKey, env_state: EnvState, action: chex.Array) -> Tuple[chex.Array, EnvState]:\n",
    "        return env.step(rng, env_state, action, env_params)\n",
    "    state_shape = env.observation_space(env_params).shape\n",
    "    n_actions = env.action_space().n\n",
    "    buffer_state = FIFOBuffer.init_buffer(buffer_size=args.fifo_buffer_size, state_shape=state_shape)\n",
    "    rng, reset_rng = jax.random.split(rng, 2)\n",
    "    obs, env_state = env_reset(reset_rng)\n",
    "    environment_step = jnp.array(0, dtype=jnp.int32)\n",
    "\n",
    "    agent_iter = partial(agent_iteration, args, SimpleDQNAgent, FIFOBuffer, env_reset, env_step)\n",
    "    \n",
    "    agent_iter = jax.jit(agent_iter, donate_argnums=(2,)).lower(\n",
    "        # states\n",
    "        rng, agent_state, buffer_state, env_state,\n",
    "        # inputs\n",
    "        obs, environment_step\n",
    "    ).compile()\n",
    "    losses = []\n",
    "    steps = []\n",
    "    returns = []\n",
    "    while environment_step < args.sample_budget:\n",
    "        rng, agent_state, buffer_state, env_state, obs, environment_step, dqn_losses, eval_returns = agent_iter(\n",
    "\n",
    "            rng, agent_state, buffer_state, env_state,\n",
    "            # inputs\n",
    "            obs, environment_step\n",
    "        )\n",
    "        # print(environment_step)\n",
    "        steps.append(environment_step)\n",
    "        returns.append(eval_returns)\n",
    "        losses.append(dqn_losses)\n",
    "        # return returns, losses\n",
    "    seeds_dqn[SEED] = (steps, returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afdbff8-5988-4069-8029-8c67401dbfe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4,3))\n",
    "plt.tight_layout()\n",
    "mean_return = np.stack([seeds_dqn[k][1] for k in seeds_dqn.keys()]).mean(0)\n",
    "std_return = np.stack([seeds_dqn[k][1] for k in seeds_dqn.keys()]).std(0)\n",
    "plt.plot(steps, mean_return, label='dqn')\n",
    "plt.fill_between(steps, mean_return - std_return, mean_return + std_return, alpha=0.3)\n",
    "plt.ylim(0, 500)\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.savefig('dqn.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ba85436-6ce8-4510-af63-677e3159dbe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds_ddqn = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0694ce94-c049-4888-aff4-9e2e8ad7d8b0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tqdm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m SEED \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m40\u001b[39m, \u001b[38;5;241m51\u001b[39m)):\n\u001b[0;32m      2\u001b[0m     args \u001b[38;5;241m=\u001b[39m DQNTrainingArgs()\n\u001b[0;32m      3\u001b[0m     rng \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mPRNGKey(SEED)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tqdm' is not defined"
     ]
    }
   ],
   "source": [
    "for SEED in tqdm(range(40, 51)):\n",
    "    args = DQNTrainingArgs()\n",
    "    rng = jax.random.PRNGKey(SEED)\n",
    "    rng, agent_init_rng = jax.random.split(rng, 2)\n",
    "    # create the agent and its optimization state\n",
    "    agent_state = DoubleDQNAgent.initialize_agent_state(DoubleDQNAgent.dqn, agent_init_rng, args)\n",
    "    # create the environment\n",
    "    env, env_params = gymnax.make('CartPole-v1')\n",
    "    def env_reset(rng: chex.PRNGKey) -> Tuple[chex.Array, EnvState]:\n",
    "        return env.reset(rng, env_params)\n",
    "    def env_step(rng: chex.PRNGKey, env_state: EnvState, action: chex.Array) -> Tuple[chex.Array, EnvState]:\n",
    "        return env.step(rng, env_state, action, env_params)\n",
    "    state_shape = env.observation_space(env_params).shape\n",
    "    n_actions = env.action_space().n\n",
    "    # create replay buffer storage\n",
    "    buffer_state = FIFOBuffer.init_buffer(buffer_size=args.fifo_buffer_size, state_shape=state_shape)\n",
    "    # reset the environment to start working with it\n",
    "    rng, reset_rng = jax.random.split(rng, 2)\n",
    "    obs, env_state = env_reset(reset_rng)\n",
    "    environment_step = jnp.array(0, dtype=jnp.int32)\n",
    "\n",
    "    # now we define the main function to update the agent and compile it\n",
    "    agent_iter = partial(agent_iteration, args, DoubleDQNAgent, FIFOBuffer, env_reset, env_step)\n",
    "    # donate_argnums=(2,) tells jax to optimize all operations with replay buffer\n",
    "    # and do them in-place\n",
    "    # the cost of this to that we need to recompile it every time we reinitialize the agent\n",
    "    # (because some functions get recreated). we could avoid this at the cost of a bit \n",
    "    # more complicated implementation.\n",
    "    agent_iter = jax.jit(agent_iter, donate_argnums=(2,)).lower(\n",
    "        # states\n",
    "        rng, agent_state, buffer_state, env_state,\n",
    "        # inputs\n",
    "        obs, environment_step\n",
    "    ).compile()\n",
    "    losses = []\n",
    "    steps = []\n",
    "    returns = []\n",
    "    while environment_step < args.sample_budget:\n",
    "        rng, agent_state, buffer_state, env_state, obs, environment_step, dqn_losses, eval_returns = agent_iter(\n",
    "            # states\n",
    "            rng, agent_state, buffer_state, env_state,\n",
    "            # inputs\n",
    "            obs, environment_step\n",
    "        )\n",
    "        # print(environment_step)\n",
    "        steps.append(environment_step)\n",
    "        returns.append(eval_returns)\n",
    "        losses.append(dqn_losses)\n",
    "        # return returns, losses\n",
    "    seeds_ddqn[SEED] = (steps, returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23603560-5a12-40ff-8992-17f49325fcf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4,3))\n",
    "plt.tight_layout()\n",
    "mean_return = np.stack([seeds_dqn[k][1] for k in seeds_dqn.keys()]).mean(0)\n",
    "std_return = np.stack([seeds_dqn[k][1] for k in seeds_dqn.keys()]).std(0)\n",
    "plt.plot(steps, mean_return, label='dqn')\n",
    "plt.fill_between(steps, mean_return - std_return, mean_return + std_return, alpha=0.3)\n",
    "\n",
    "mean_return = np.stack([seeds_ddqn[k][1] for k in seeds_ddqn.keys()]).mean(0)\n",
    "std_return = np.stack([seeds_ddqn[k][1] for k in seeds_ddqn.keys()]).std(0)\n",
    "plt.plot(steps, mean_return, label='double-dqn')\n",
    "plt.fill_between(steps, mean_return - std_return, mean_return + std_return, alpha=0.3)\n",
    "\n",
    "plt.ylim(0, 500)\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.savefig('double-dqn.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7287f431-b846-4b8a-b3c3-b894e9adcb88",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
